[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Weather Model Methods and Results",
    "section": "",
    "text": "Our observations are obtained from Castro (2023) and contain U.S. weather stations and SNOTEL stations, providing data in the period from 1 January 2017 to 21 September 2017. Columns of the observations include\n\n\n\n\n\n\n\n\nName\nData Type\nDescription\n\n\n\n\nstation\nstring\nName of the weather station\n\n\nstate\nstring\nState/province of the weather station\n\n\nlatitude\ndouble\nLatitude of the weather station (degrees)\n\n\nlongitude\ndouble\nLongitude of the weather station\n\n\nelevation\ndouble\nElevation of the weather station (meters)\n\n\ndate\nstring\nDate of the observation\n\n\ntemp_min\ndouble\nMinimum daily temperature (fahrenheit)\n\n\ntemp_max\ndouble\nMaximum daily temperature (fahrenheit)\n\n\ntemp_avg\ndouble\nAverage daily temperature (fahrenheit)\n\n\nav_day_wi_spd\ndouble\nAverage daily wind speed\n\n\nwi_dir_5_sec\ndouble\nWind direction at 5 second intervals (degrees)\n\n\nwi_spd_5_sec\ndouble\nWind speed at 5 seconds intervals\n\n\nsnow_fall\ndouble\nSnow water equivalence\n\n\nsnow_dep\ndouble\nSnow depth\n\n\nprecip\ndouble\nPrecipitation\n\n\n\nOur interest is in creating a model from the available data using\n\nlatitude\nlongitude\nelevation\ndate\n\nas predictors of temp_avg. To prepare the data for modeling, we first load the data.\n\nweather &lt;- read.csv(\"./data/weather.csv\")\n\nNext, we remove columns from the dataframe we are not interested in.\n\n# Keep only our x and y columns from the data table\nweather &lt;- weather[c(\"date\", \"longitude\", \"latitude\", \"elevation\", \"temp_avg\")]\n\n# Drop observations with NA values for our x and y columns\nweather &lt;- weather %&gt;% drop_na()\n\nSince the date is not useful as a string for fitting a model, we replace it with the number of days since our first observations on 1 January 2017.\n\n# Convert \"date\" column from string to date objects\nweather$date &lt;- as.Date(as.character(weather$date), format=\"%Y%m%d\")\n\n# Create new column counting the days since Jan 1 2017\nweather$day &lt;- as.numeric(difftime(weather$date, as.Date(\"20170101\", format=\"%Y%m%d\"), units=\"days\"))\n\n# Remove date column\nweather &lt;- weather[c(\"day\", \"longitude\", \"latitude\", \"elevation\", \"temp_avg\")]\n\nFinally, we restrict our data to the lower 48 U.S. states and the District of Columbia. To accomplish this, we must:\n\nConvert our data.frame into a geospatial sf data frame object\nLoad shapefiles for the area of interest\nExclude observations spatially located outside of the shapefile\nConvert the sf back into a data.frame object\n\nThe process for this follows below.\n\n# Convert our data frame to sf\nweather_sf &lt;- st_as_sf(weather, coords = c(\"longitude\", \"latitude\"), crs=st_crs(\"EPSG:4269\"))\n\n# Load U.S. States shapefile\nstates_sf &lt;- read_sf(\"./data/cb_2018_us_state_500k/cb_2018_us_state_500k.shp\", crs=st_crs(\"EPSG:4269\"))\n\n# Exclude specific states and territories\nstates_sf &lt;- subset(states_sf, !(STATEFP %in% c(\"02\",\"15\",\"60\",\"66\",\"69\",\"72\",\"78\")))\n\n# Remove observations outside the remaining states and territories\nweather_sf &lt;- st_intersection(weather_sf, st_union(states_sf))\n\n# Convert back into data frame\nweather &lt;- st_drop_geometry(weather_sf)\nweather$longitude &lt;- st_coordinates(weather_sf)[,\"X\"]\nweather$latitude &lt;- st_coordinates(weather_sf)[,\"Y\"]\n\nWe now have data to which we may properly fit our model.\n\n\n\n  \n\n\n\nBefore continuing, it is worth noticing the spatial distribution of the observations in Figure 1. Specifically, there is a much higher distribution of observations in the inter-mountain west, which has implications for our model. These will be discussed in more detail later.\n\n\n\n\n\n\n\n\nFigure 1: Spatial distribution of the observations within the data set.\n\n\n\n\n\nWe may plot the data in two dimensions at a time and see what kind of trends, if any, exist in the data. First, looking at the effect of latitude on temp_avg\n\n\n\n\n\n\n\n\n\n\n\n(a) Average daily temperature versus time for all days.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Average daily temperature versus elevation for all days.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Average daily temperature versus latitude for all days.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(d) Average daily temperature versus longitude for all days.\n\n\n\n\n\n\n\nFigure 2: Comparison of predictors elevation, days, latitude and longitude versus outcome temp_avg.\n\n\n\nFirst looking at the data, it seems trends do exist in the data. Looking at Figure 2 (b), there is a clear and negative linear correlation between elevation and temp_avg Additionally, there is a clear and negative linear correlation between latitude and temp_avg. The two remaining predictors, longitude and days, also appear to have some type of trend, but it is not clear that they are linear.\nBased from physical understanding of the weather, the correlation both latitude and elevation have are intuitive. First, atmospheric pressure decreases in proportion to elevation. In turn, Gay-Lussac’s law informs us that \\[P \\propto T.\\] Therefore, we transitively expect temperature to decrease in proportion to elevation, all else being equal.\nIn addition, background knowledge about solar irradiance informs us that irradiance, which is the primary forcing effect in surface temperature, decreases in intensity in proportion to distance from the solar equator, due to the angle of incidence at which solar rays shine on the Earth.\n\n\n\nDemontration of the relationship between angle of incidence and intensity of solar irradiance.\n\n\nIgnoring the effect of the atmosphere on solar irradiance, geometric reasoning tells us that solar itensity \\(I\\) at a given latitude, where \\(I_0\\) is the intensity at the equator, is \\[ I = I_0 \\cos{\\theta}.\\] However, in our data, where our latitude varies from approximately 25 to 50 degrees, this function \\(I\\) is roughly linear, as shown in Figure 3.\n\n\n\n\n\n\n\n\nFigure 3: Theoretical irradiance versus latitude in the lower 48 states.\n\n\n\n\n\nIn addition, if we interpolate the data, we may view how this data changes in space and over time, though the effect of elevation is not visualized. The inverse weighted interpolation is computationally intensive, and so patience is required to generate the resulting visualization in Figure 4.\n\n\n\n\n\n\nFigure 4: Inverse distance weighted interpolation of temp_avg from each station for each day in the data set.\n\n\n\n\n\n\nOwing to the fact that this model is a combination of both linear and highly non-linear physical effects, we choose to fit the data set with a Generalized Additive Model, implemented by Hastie (2023). In this type of model, we explain our outcome variable \\(\\hat{y}\\) as the sum of smooth functions on our predictors \\(x_i\\), i.e.\n\\[\\hat{y} = \\beta_0 + f_1(x_1) + f_2(x_2) + \\cdots + f_n(x_n).\\] For our particular predictors, we use a simple linear function \\(f_i(x_i) = \\beta_ix_i\\) for elevation, latitude and longitude, and a nonlinear smooth function for days. Then, our particular model is\n\\[\\text{temp_avg} = \\beta_0 + \\beta_1 (\\text{elevation}) + \\beta_2 (\\text{latitude}) + \\beta_3 (\\text{longitude}) + f(\\text{days}).\\] since we hypothesize the partial derivatives of temp_avg with respect to longitude, latitude and elevation to be roughly constant.\nBefore we perform regression, we first split the the data in weather randomly into training and testing sets with a 7 to 3 split.\n\n# Randomly choose our indices to sample from for our training set\nsample_frac &lt;- 0.7\ntrain_inds &lt;- sample(c(TRUE, FALSE), nrow(weather), replace=TRUE, \n                     prob=c(sample_frac,1-sample_frac))\n\n# Create training set from training set indices\ntrain &lt;- weather[train_inds,]\ntrain_x &lt;- train[, c('elevation','longitude','latitude','day')]\ntrain_y &lt;- train[, c('temp_avg')]\n\n# Create testing set from negation of training set indices\ntest &lt;- weather[!train_inds,]\ntest_x &lt;- test[, c('elevation','longitude','latitude','day')]\ntest_y &lt;- test[, c('temp_avg')]\n\nNow we fit our model.\n\nlibrary(\"gam\")\n\ngam_weather &lt;- gam(temp_avg ~ elevation + longitude + latitude + s(day,df=5), data=train)"
  },
  {
    "objectID": "index.html#observations",
    "href": "index.html#observations",
    "title": "Weather Model Methods and Results",
    "section": "",
    "text": "Our observations are obtained from Castro (2023) and contain U.S. weather stations and SNOTEL stations, providing data in the period from 1 January 2017 to 21 September 2017. Columns of the observations include\n\n\n\n\n\n\n\n\nName\nData Type\nDescription\n\n\n\n\nstation\nstring\nName of the weather station\n\n\nstate\nstring\nState/province of the weather station\n\n\nlatitude\ndouble\nLatitude of the weather station (degrees)\n\n\nlongitude\ndouble\nLongitude of the weather station\n\n\nelevation\ndouble\nElevation of the weather station (meters)\n\n\ndate\nstring\nDate of the observation\n\n\ntemp_min\ndouble\nMinimum daily temperature (fahrenheit)\n\n\ntemp_max\ndouble\nMaximum daily temperature (fahrenheit)\n\n\ntemp_avg\ndouble\nAverage daily temperature (fahrenheit)\n\n\nav_day_wi_spd\ndouble\nAverage daily wind speed\n\n\nwi_dir_5_sec\ndouble\nWind direction at 5 second intervals (degrees)\n\n\nwi_spd_5_sec\ndouble\nWind speed at 5 seconds intervals\n\n\nsnow_fall\ndouble\nSnow water equivalence\n\n\nsnow_dep\ndouble\nSnow depth\n\n\nprecip\ndouble\nPrecipitation\n\n\n\nOur interest is in creating a model from the available data using\n\nlatitude\nlongitude\nelevation\ndate\n\nas predictors of temp_avg. To prepare the data for modeling, we first load the data.\n\nweather &lt;- read.csv(\"./data/weather.csv\")\n\nNext, we remove columns from the dataframe we are not interested in.\n\n# Keep only our x and y columns from the data table\nweather &lt;- weather[c(\"date\", \"longitude\", \"latitude\", \"elevation\", \"temp_avg\")]\n\n# Drop observations with NA values for our x and y columns\nweather &lt;- weather %&gt;% drop_na()\n\nSince the date is not useful as a string for fitting a model, we replace it with the number of days since our first observations on 1 January 2017.\n\n# Convert \"date\" column from string to date objects\nweather$date &lt;- as.Date(as.character(weather$date), format=\"%Y%m%d\")\n\n# Create new column counting the days since Jan 1 2017\nweather$day &lt;- as.numeric(difftime(weather$date, as.Date(\"20170101\", format=\"%Y%m%d\"), units=\"days\"))\n\n# Remove date column\nweather &lt;- weather[c(\"day\", \"longitude\", \"latitude\", \"elevation\", \"temp_avg\")]\n\nFinally, we restrict our data to the lower 48 U.S. states and the District of Columbia. To accomplish this, we must:\n\nConvert our data.frame into a geospatial sf data frame object\nLoad shapefiles for the area of interest\nExclude observations spatially located outside of the shapefile\nConvert the sf back into a data.frame object\n\nThe process for this follows below.\n\n# Convert our data frame to sf\nweather_sf &lt;- st_as_sf(weather, coords = c(\"longitude\", \"latitude\"), crs=st_crs(\"EPSG:4269\"))\n\n# Load U.S. States shapefile\nstates_sf &lt;- read_sf(\"./data/cb_2018_us_state_500k/cb_2018_us_state_500k.shp\", crs=st_crs(\"EPSG:4269\"))\n\n# Exclude specific states and territories\nstates_sf &lt;- subset(states_sf, !(STATEFP %in% c(\"02\",\"15\",\"60\",\"66\",\"69\",\"72\",\"78\")))\n\n# Remove observations outside the remaining states and territories\nweather_sf &lt;- st_intersection(weather_sf, st_union(states_sf))\n\n# Convert back into data frame\nweather &lt;- st_drop_geometry(weather_sf)\nweather$longitude &lt;- st_coordinates(weather_sf)[,\"X\"]\nweather$latitude &lt;- st_coordinates(weather_sf)[,\"Y\"]\n\nWe now have data to which we may properly fit our model.\n\n\n\n  \n\n\n\nBefore continuing, it is worth noticing the spatial distribution of the observations in Figure 1. Specifically, there is a much higher distribution of observations in the inter-mountain west, which has implications for our model. These will be discussed in more detail later.\n\n\n\n\n\n\n\n\nFigure 1: Spatial distribution of the observations within the data set.\n\n\n\n\n\nWe may plot the data in two dimensions at a time and see what kind of trends, if any, exist in the data. First, looking at the effect of latitude on temp_avg\n\n\n\n\n\n\n\n\n\n\n\n(a) Average daily temperature versus time for all days.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Average daily temperature versus elevation for all days.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Average daily temperature versus latitude for all days.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(d) Average daily temperature versus longitude for all days.\n\n\n\n\n\n\n\nFigure 2: Comparison of predictors elevation, days, latitude and longitude versus outcome temp_avg.\n\n\n\nFirst looking at the data, it seems trends do exist in the data. Looking at Figure 2 (b), there is a clear and negative linear correlation between elevation and temp_avg Additionally, there is a clear and negative linear correlation between latitude and temp_avg. The two remaining predictors, longitude and days, also appear to have some type of trend, but it is not clear that they are linear.\nBased from physical understanding of the weather, the correlation both latitude and elevation have are intuitive. First, atmospheric pressure decreases in proportion to elevation. In turn, Gay-Lussac’s law informs us that \\[P \\propto T.\\] Therefore, we transitively expect temperature to decrease in proportion to elevation, all else being equal.\nIn addition, background knowledge about solar irradiance informs us that irradiance, which is the primary forcing effect in surface temperature, decreases in intensity in proportion to distance from the solar equator, due to the angle of incidence at which solar rays shine on the Earth.\n\n\n\nDemontration of the relationship between angle of incidence and intensity of solar irradiance.\n\n\nIgnoring the effect of the atmosphere on solar irradiance, geometric reasoning tells us that solar itensity \\(I\\) at a given latitude, where \\(I_0\\) is the intensity at the equator, is \\[ I = I_0 \\cos{\\theta}.\\] However, in our data, where our latitude varies from approximately 25 to 50 degrees, this function \\(I\\) is roughly linear, as shown in Figure 3.\n\n\n\n\n\n\n\n\nFigure 3: Theoretical irradiance versus latitude in the lower 48 states.\n\n\n\n\n\nIn addition, if we interpolate the data, we may view how this data changes in space and over time, though the effect of elevation is not visualized. The inverse weighted interpolation is computationally intensive, and so patience is required to generate the resulting visualization in Figure 4.\n\n\n\n\n\n\nFigure 4: Inverse distance weighted interpolation of temp_avg from each station for each day in the data set."
  },
  {
    "objectID": "index.html#procedure",
    "href": "index.html#procedure",
    "title": "Weather Model Methods and Results",
    "section": "",
    "text": "Owing to the fact that this model is a combination of both linear and highly non-linear physical effects, we choose to fit the data set with a Generalized Additive Model, implemented by Hastie (2023). In this type of model, we explain our outcome variable \\(\\hat{y}\\) as the sum of smooth functions on our predictors \\(x_i\\), i.e.\n\\[\\hat{y} = \\beta_0 + f_1(x_1) + f_2(x_2) + \\cdots + f_n(x_n).\\] For our particular predictors, we use a simple linear function \\(f_i(x_i) = \\beta_ix_i\\) for elevation, latitude and longitude, and a nonlinear smooth function for days. Then, our particular model is\n\\[\\text{temp_avg} = \\beta_0 + \\beta_1 (\\text{elevation}) + \\beta_2 (\\text{latitude}) + \\beta_3 (\\text{longitude}) + f(\\text{days}).\\] since we hypothesize the partial derivatives of temp_avg with respect to longitude, latitude and elevation to be roughly constant.\nBefore we perform regression, we first split the the data in weather randomly into training and testing sets with a 7 to 3 split.\n\n# Randomly choose our indices to sample from for our training set\nsample_frac &lt;- 0.7\ntrain_inds &lt;- sample(c(TRUE, FALSE), nrow(weather), replace=TRUE, \n                     prob=c(sample_frac,1-sample_frac))\n\n# Create training set from training set indices\ntrain &lt;- weather[train_inds,]\ntrain_x &lt;- train[, c('elevation','longitude','latitude','day')]\ntrain_y &lt;- train[, c('temp_avg')]\n\n# Create testing set from negation of training set indices\ntest &lt;- weather[!train_inds,]\ntest_x &lt;- test[, c('elevation','longitude','latitude','day')]\ntest_y &lt;- test[, c('temp_avg')]\n\nNow we fit our model.\n\nlibrary(\"gam\")\n\ngam_weather &lt;- gam(temp_avg ~ elevation + longitude + latitude + s(day,df=5), data=train)"
  },
  {
    "objectID": "index.html#limitations",
    "href": "index.html#limitations",
    "title": "Weather Model Methods and Results",
    "section": "3.1 Limitations",
    "text": "3.1 Limitations\n\n3.1.1 Limitations of the Data\nSeveral limitations exist on what may and may not be concluded from this data and the analysis employed. For one, the data is limited to only part of the year 2017, and geographically constrained to the lower 48 states of the United States. As a result, all of the observed trends cannot be extrapolated beyond this region and this year without employing outside knowledge from the climate sciences.\nWith this caveat in mind, this data does corroborate and certain known phenomenon we hypothesized earlier would be shown; in particular that\n\nTemperature decreases with proportion to elevation\nTemperature increases with proportion to absolute difference in latitude from the solar equator\n\nIn addition, linear modeling of the trend across longitude suggested that, across the lower 48 states, temperature decreased with proportion to longitude. In general, this is a trend driven by an array of confounding atmospheric and climatological phenomenon, in particular\n\nJet streams\nNorth American topology\nOcean currents\n\nfor which data is not included in the study, and thus these causes cannot be identified without analysis of additional sets of data.\nLastly, there are some limitations in the data, specifically resulting from the spatial distribution of our observations, as shown in Figure 1. Because of the over representation of observations in the inter-mountain west region, our error is greatest in the eastern US, where stations are lower in density. This becomes particularly evident around day 150, as shown in Figure 6, where much of the residuals in the west are small, and but predicting colder weather than is actually observed in most of the eastern US.\n\n\n3.1.2 Limitations of the Model\nWeather systems and atmospheric dynamics are best explained through fluid dynamics and simulations through global circulation models (GCMs). Fitting any linear or nonlinear model will inherently result in a model that cannot be generalized outside the time or region. However, our model still remains useful as a tool for observing inner-year trends across the geographical region(s) and time period(s) observed."
  },
  {
    "objectID": "index.html#interpolation-code",
    "href": "index.html#interpolation-code",
    "title": "Weather Model Methods and Results",
    "section": "6.1 Interpolation Code",
    "text": "6.1 Interpolation Code\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(gstat)\nlibrary(raster)\nlibrary(gganimate)\nlibrary(av)\n\n# Get the first and last day\nday_min &lt;- min(weather_sf$day)\nday_max &lt;- max(weather_sf$day)\nbbox &lt;- st_bbox(st_union(states_sf))\n\n# Create a list of rasters\nrasters &lt;- list()\n\nfor (d in day_min:day_max) {\n  # Choose weather data from a single day\n  daily &lt;- subset(weather_sf, day == d)\n  date &lt;- as.Date(\"20170101\", format=\"%Y%m%d\") + hms(\"24:00:00\")*d\n  \n  # Interpolate the weather station data\n  gs &lt;- gstat::gstat(formula=temp_avg~1, data=daily, nmax=5, set=list(idp = 0))\n  grid &lt;- raster::raster(ncol=200, nrow=200, xmn=bbox$xmin, xmx=bbox$xmax, ymn=bbox$ymin, ymx=bbox$ymax, crs=\"EPSG:4269\")\n  \n  # Create a raster from the interpolated data\n  rs &lt;- raster::interpolate(grid, model=gs)\n  rsm &lt;- raster::mask(rs, as_Spatial(st_union(weather_sf)))\n  \n  # Save the result for the day\n  file &lt;- writeRaster(rsm, filename = paste('./avgdaily/', d, \".tif\", sep=\"\"), format=\"GTiff\", overwrite=TRUE)\n}\n\n# Create a list of the rasters and form a raster brick\nrasters &lt;- list()\nfor (d in day_min:day_max) { rasters[[length(rasters)+1]] &lt;- raster(paste(\"./avgdaily/\", d, \".tif\", sep=\"\"))}\nbr &lt;- raster::brick(rasters)\n\n# Create a dataframe from the  raster brick\nrsm_df &lt;- purrr::map_dfr(\n  as.list(br),\n  ~setNames(as.data.frame(as(., \"SpatialPixelsDataFrame\")), c('temp_avg', 'x', 'y')),\n  .id = 'day'\n)\nrsm_df$day &lt;- as.numeric(rsm_df$day)\n\n# Plot the data\ngga &lt;- ggplot() + \n  geom_tile(data=rsm_df, mapping=aes(x=x,y=y,fill=temp_avg)) +\n  scale_fill_viridis_b(name=\"Temperature (F)\", na.value = \"transparent\", limits=c(temp_avg_min, temp_avg_max)) +\n  labs(x = \"Longitude\", y = \"Latitude\") +\n  geom_sf(data=L48_states_sf, alpha=0) +\n  theme_minimal() +\n  gganimate::transition_time(as.numeric(day)) +\n  ggtitle(\"Average Daily Temperature (Day {frame-1})\")\n\ngganim &lt;- gganimate::animate(gga, renderer=av_renderer(), nframes=day_max, height=1080, width=1920, units=\"px\", res=300)\nanim_save(\"./avgdaily.mp4\", gganim)"
  }
]